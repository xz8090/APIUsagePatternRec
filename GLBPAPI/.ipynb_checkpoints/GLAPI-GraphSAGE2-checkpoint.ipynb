{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle as pk\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from stellargraph.data import UnsupervisedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator,GraphSAGELinkGenerator\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.layer import GraphSAGE, HinSAGE, link_classification\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "\n",
    "from stellargraph import globalvar\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "datasetname = 'TEST'#有数据集SH_S、SH_L、MV\n",
    "threshold1 = 0.5\n",
    "threshold2 = 0.5\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "num_samples = [30, 20]\n",
    "layer_sizes = [64, 64]\n",
    "\n",
    "best_suc = [0]*21\n",
    "best_pre = [0]*21\n",
    "best_recall = [0]*21\n",
    "pro_best_suc = [0]*21\n",
    "pro_best_pre = [0]*21\n",
    "pro_best_recall = [0]*21\n",
    "test_config = 'C2.1'\n",
    "\n",
    "def getg(data,minsup=1):\n",
    "    g = nx.Graph()\n",
    "    n = len(data.item_method_id)\n",
    "    # 顶点\n",
    "    point = []\n",
    "    for i in range(n):\n",
    "        point.append(i)\n",
    "    g.add_nodes_from(point)\n",
    "    # 边权重\n",
    "    edglist = []\n",
    "    edges = set()\n",
    "    for user, items in tqdm(data.invocation_mx.items()):\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i+1,len(items)):\n",
    "                edges.add((items[i],items[j]))\n",
    "    \n",
    "    for edg in tqdm(edges):\n",
    "        weight = float(data.adj[edg[0],edg[1]])\n",
    "        if weight>=minsup:\n",
    "            edglist.append((edg[0],edg[1],weight))\n",
    "        #edglist.append((edg[0],edg[1]))\n",
    "\n",
    "    g.add_weighted_edges_from(edglist)\n",
    "    #g.add_edges_from(edglist)\n",
    "    return g\n",
    "\n",
    "def to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def build_my_new_adj_matrix(data,train_dict):\n",
    "    n = len(data.item_method_id)\n",
    "    A = sp.dok_matrix((n, n), dtype=np.float32)\n",
    "    FD = defaultdict(int)\n",
    "    for user, items in tqdm(train_dict.items()):\n",
    "        for i in range(len(items)):\n",
    "            FD[items[i]] += 1\n",
    "            for j in range(i+1,len(items)):\n",
    "                if A[items[i],items[j]] == 0:\n",
    "                    A[items[i],items[j]] = A[items[j],items[i]] = len(data.invocation_mx.items())\n",
    "                else:\n",
    "                    A[items[i], items[j]] = A[items[j], items[i]] = A[items[j], items[i]] + len(data.invocation_mx.items())\n",
    "    print('build_my_new_adj_matrix finish')\n",
    "    data.FD = FD\n",
    "    data.adj = to_torch_sparse_tensor(A)\n",
    "    \n",
    "\n",
    "def load_mydata(dataset_name):\n",
    "    name = './tmp/%s-mydata.pk' % dataset_name\n",
    "    if not os.path.exists(name):\n",
    "        print('no file.')\n",
    "    with open(name, 'rb') as f:\n",
    "        data = pk.load(f)\n",
    "        print('load dataset from disk.')\n",
    "    #data.adj = to_torch_sparse_tensor(data.adj)\n",
    "    return data\n",
    "\n",
    "def get_node_embeddings(data):\n",
    "    g = getg(data)\n",
    "    g_feature_attr = g.copy()\n",
    "    for node_id, node_data in g_feature_attr.nodes(data=True):\n",
    "        node_data[\"feature\"] = data.item_pre_emb[node_id].numpy()\n",
    "\n",
    "    G = StellarGraph.from_networkx(\n",
    "        g_feature_attr, node_features=\"feature\", node_type_default=\"API\", edge_type_default=\"Attribute\"\n",
    "    )\n",
    "    edge_splitter_test = EdgeSplitter(G)\n",
    "\n",
    "    # Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G, and obtain the\n",
    "    # reduced graph G_test with the sampled links removed:\n",
    "    G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
    "        p=0.1, method=\"global\", keep_connected=True\n",
    "    )\n",
    "    # Define an edge splitter on the reduced graph G_test:\n",
    "    edge_splitter_train = EdgeSplitter(G_test)\n",
    "\n",
    "    # Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
    "    # reduced graph G_train with the sampled links removed:\n",
    "    G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
    "        p=0.1, method=\"global\", keep_connected=True\n",
    "    )\n",
    "\n",
    "    train_gen = GraphSAGELinkGenerator(G_train, batch_size, num_samples)\n",
    "    train_flow = train_gen.flow(edge_ids_train, edge_labels_train, shuffle=True)\n",
    "    test_gen = GraphSAGELinkGenerator(G_test, batch_size, num_samples)\n",
    "    test_flow = test_gen.flow(edge_ids_test, edge_labels_test)\n",
    "    graphsage = GraphSAGE(\n",
    "        layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3\n",
    "    )\n",
    "    x_inp, x_out = graphsage.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"relu\", edge_embedding_method=\"ip\"\n",
    "    )(x_out)\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "    history = model.fit(train_flow, epochs=epochs, validation_data=test_flow, verbose=2)\n",
    "    x_inp_src = x_inp[0::2]\n",
    "    x_out_src = x_out[0]\n",
    "    embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)\n",
    "    node_ids = G_train.nodes()\n",
    "    node_gen = GraphSAGENodeGenerator(G_train, batch_size, num_samples).flow(node_ids)\n",
    "    node_embeddings = embedding_model.predict(node_gen, workers=4, verbose=1)\n",
    "    return node_embeddings\n",
    "\n",
    "#threshold1取值（0，1）表示考虑节点相似特征的阈值，值越大候选特征节点越少\n",
    "#threshold2取值（0，1）表示节点属性特征的重要性，越小越不重要\n",
    "def build_new_relation(ratings,threshold1 = 0.8,threshold2 = 0.001):\n",
    "    dataset = load_mydata(datasetname)\n",
    "    n = len(ratings)\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(i+1,n):\n",
    "            simily = float(ratings[i][j])\n",
    "            if simily >= threshold1:\n",
    "                dataset.adj[i,j] = threshold2*simily\n",
    "    return dataset\n",
    "    \n",
    "def get_my_top_items(tensor):\n",
    "    item_dict = {}\n",
    "    for i in tqdm(range(len(tensor))):\n",
    "        if tensor[i].item() !=0 :\n",
    "            item_dict[i] = tensor[i].item()\n",
    "    #print('get_my_top_items==>item_dict',item_dict)\n",
    "    top_items = [item[0] for item in sorted(item_dict.items(),key=lambda item:item[1],reverse=True)]\n",
    "    #print('get_my_top_items==>top_items',top_items)\n",
    "    return top_items[:21]\n",
    "\n",
    "def get_my_top_items2(data,adj,Q,ratings,a,b):\n",
    "    #链接预测\n",
    "    diag = torch.diag(ratings) #获取对角为一维向量\n",
    "    diag_embed = torch.diag_embed(diag)  # 由diag恢复为对角矩阵\n",
    "    link_embed = ratings - diag_embed\n",
    "    rowsoftmax = F.softmax(link_embed,dim=1)\n",
    "    link_q1 = torch.zeros(size=[len(rowsoftmax[0])])\n",
    "    for q in Q:\n",
    "        link_q1 = link_q1 + rowsoftmax[q]\n",
    "        \n",
    "    #贝叶斯预测\n",
    "    M = len(data.invocation_mx.items())\n",
    "    D = set()\n",
    "    FD = data.FD\n",
    "    \n",
    "    for q in Q:\n",
    "        tensor = adj[q]\n",
    "        for i in range(len(tensor)):\n",
    "            if tensor[i].item() != 0:\n",
    "                D.add(i)\n",
    "\n",
    "    link_q2 = torch.zeros(size=[len(adj)])\n",
    "    print(len(link_q2))\n",
    "    for d in D:\n",
    "        fd = FD[d]\n",
    "        fdq = 1\n",
    "        for q in Q:\n",
    "            tensor = adj[q]\n",
    "            fdq = fdq*(tensor[d].item()*1.0/fd)\n",
    "        #利用贝叶斯求得d被预测的概率\n",
    "        p2 = fdq*fd*1.0/M\n",
    "        link_q2[d] = p2\n",
    "    \n",
    "\n",
    "    link_q = link_q1*a+link_q2*b\n",
    "    arr,top_items = torch.sort(link_q,descending=True)\n",
    "    top_items = top_items[:21]\n",
    "    #top_items = [item[0] for item in sorted(item_dict.items(), key=lambda item: item[1], reverse=True)]\n",
    "    # print('get_my_top_items==>top_items',top_items)\n",
    "    return top_items.numpy()\n",
    "\n",
    "def myeval(dataset,ratings,a,b):\n",
    "    test_set = dataset.test_dict\n",
    "    logger.info('test start. test set size: %d' % len(test_set))\n",
    "    t1 = time.time()\n",
    "    users = np.asarray(list(test_set.keys()))  # 训练集的方法编号数组\n",
    "\n",
    "    top_items = []\n",
    "    used_items = []\n",
    "\n",
    "    for userid in tqdm(users):\n",
    "        used_items.append(set(dataset.train_dict[userid]))\n",
    "        #print(dataset.train_dict[userid],dataset.train_dict[userid][0])\n",
    "        #top_items.append(get_my_top_items(dataset.adj[dataset.train_dict[userid][0]]))\n",
    "        top_items.append(get_my_top_items2(dataset,dataset.adj,dataset.train_dict[userid],ratings,a,b))\n",
    "\n",
    "    #print('myeval2=>top_items',top_items)\n",
    "    #print('myeval2=>used_items', used_items)\n",
    "\n",
    "    items = []\n",
    "    for i, item in enumerate(top_items):  # 第i个测试方法推荐的API列表item\n",
    "        logger.info('context:%s;rank list:%s' % (str(used_items[i]),str(item)))\n",
    "        #print('context:',used_items[i],'rank list:',item)\n",
    "        # if i<=20:\n",
    "        rec_item = [tid for tid in item if tid not in used_items[i]]\n",
    "        # print(rec_item)\n",
    "        items.append(rec_item[:20])\n",
    "\n",
    "    def getMAP(N):\n",
    "        qarr = []\n",
    "        for i, uid in enumerate(users):\n",
    "            r = 0\n",
    "            drarr = []\n",
    "            for k in range(1, N+1):\n",
    "                intersect = set(items[i][:k]) & set(test_set[uid])\n",
    "                p = len(intersect) / k\n",
    "                newr = len(intersect) / len(set(test_set[uid]))\n",
    "                dr = (newr-r)*p\n",
    "                drarr.append(dr)\n",
    "                r = newr\n",
    "            qarr.append(np.sum(drarr))\n",
    "        return np.sum(qarr)/len(qarr)\n",
    "     \n",
    "    def res_at_k(k):\n",
    "        suc_methods = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        proj_suc = defaultdict(list)\n",
    "        proj_pre = defaultdict(list)\n",
    "        proj_recall = defaultdict(list)\n",
    "\n",
    "        for i, uid in enumerate(users):\n",
    "            pid = dataset.test_user2proj[uid]\n",
    "            intersect = set(items[i][:k]) & set(test_set[uid])\n",
    "            if len(intersect) > 0:\n",
    "                suc_methods.append(uid)\n",
    "                proj_suc[pid].append(1)\n",
    "            else:\n",
    "                logger.debug('failed uid %d' % uid)\n",
    "                logger.debug('GT:{}, REC:{}'.format(test_set[uid], items[i]))\n",
    "                proj_suc[pid].append(0)\n",
    "            p = len(intersect) / k\n",
    "            r = len(intersect) / len(set(test_set[uid]))\n",
    "            precisions.append(p)\n",
    "            recalls.append(r)\n",
    "            proj_pre[pid].append(p)\n",
    "            proj_recall[pid].append(r)\n",
    "        suc_rate = len(suc_methods) / len(users)\n",
    "        logger.info('----------------------result@%d--------------------------' % k)\n",
    "        logger.info('success rate at method level %f' % (suc_rate))\n",
    "        logger.info('mean precision:%f, mean recall:%f' % (np.mean(precisions), np.mean(recalls)))\n",
    "\n",
    "        suc_project = [np.mean(val) for val in proj_suc.values()]\n",
    "        pres = [np.mean(val) for val in proj_pre.values()]\n",
    "        recs = [np.mean(val) for val in proj_recall.values()]\n",
    "        logger.info('**********************************************************')\n",
    "        logger.info('success rate at project level %f' % (np.mean(np.mean(suc_project))))\n",
    "        logger.info('mean precision:%f, mean recall:%f' % (np.mean(pres), np.mean(recs)))\n",
    "        return suc_rate, np.mean(precisions), np.mean(recalls),np.mean(np.mean(suc_project)),np.mean(pres), np.mean(recs)\n",
    "\n",
    "    t2 = time.time()\n",
    "    logger.info('test end time: {}s'.format(t2 - t1))\n",
    "    for i in range(1, 21):\n",
    "        suc, pre, rec, pro_suc, pro_pre, pro_rec = res_at_k(i)\n",
    "        if suc > best_suc[i]:\n",
    "            best_suc[i] = suc\n",
    "        if pre > best_pre[i]:\n",
    "            best_pre[i] = pre\n",
    "        if rec > best_recall[i]:\n",
    "            best_recall[i] = rec\n",
    "        logger.warning('method level => top %d : best suc %f, best pre %f,  best recall %f' % (i, best_suc[i], best_pre[i], best_recall[i]))\n",
    "\n",
    "        if pro_suc > pro_best_suc[i]:\n",
    "            pro_best_suc[i] = pro_suc\n",
    "        if pro_pre > pro_best_pre[i]:\n",
    "            pro_best_pre[i] = pro_pre\n",
    "        if pro_rec > pro_best_recall[i]:\n",
    "            pro_best_recall[i] = pro_rec\n",
    "        logger.warning('project level => top %d : best suc %f, best pre %f,  best recall %f' % (i, pro_best_suc[i], pro_best_pre[i], pro_best_recall[i]))\n",
    "\n",
    "    logger.info('MAP: %f' % (getMAP(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1301/1301 [00:00<00:00, 168883.34it/s]\n",
      "  9%|▉         | 114/1301 [00:00<00:01, 1053.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from disk.\n",
      "total user methods:1301, test_proj:{8, 2}\n",
      "test set methods count:21, invocations:218\n",
      "load train datas ...\n",
      "train set methods count:1301, invocation: 34667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1301/1301 [00:01<00:00, 727.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_my_new_adj_matrix finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#载入数据并划分数据集\n",
    "data = load_mydata(datasetname)\n",
    "data.split_data(test_config)\n",
    "build_my_new_adj_matrix(data,data.train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1301/1301 [00:00<00:00, 96237.98it/s]\n",
      "100%|██████████| 22844/22844 [00:03<00:00, 5740.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 2043 positive and 2043 negative edges. **\n",
      "** Sampled 1839 positive and 1839 negative edges. **\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "29/29 - 24s - loss: 0.7402 - binary_accuracy: 0.5288 - val_loss: 2.2000 - val_binary_accuracy: 0.5272\n",
      "Epoch 2/20\n",
      "29/29 - 22s - loss: 0.7108 - binary_accuracy: 0.5258 - val_loss: 2.0215 - val_binary_accuracy: 0.5272\n",
      "Epoch 3/20\n",
      "29/29 - 22s - loss: 0.6846 - binary_accuracy: 0.5487 - val_loss: 1.9810 - val_binary_accuracy: 0.5272\n",
      "Epoch 4/20\n",
      "29/29 - 23s - loss: 0.6705 - binary_accuracy: 0.5696 - val_loss: 1.7991 - val_binary_accuracy: 0.5803\n",
      "Epoch 5/20\n",
      "29/29 - 22s - loss: 0.6540 - binary_accuracy: 0.5780 - val_loss: 1.5936 - val_binary_accuracy: 0.5915\n",
      "Epoch 6/20\n",
      "29/29 - 22s - loss: 0.6271 - binary_accuracy: 0.6069 - val_loss: 1.4904 - val_binary_accuracy: 0.6180\n",
      "Epoch 7/20\n",
      "29/29 - 22s - loss: 0.6138 - binary_accuracy: 0.6161 - val_loss: 1.3870 - val_binary_accuracy: 0.6243\n",
      "Epoch 8/20\n",
      "29/29 - 22s - loss: 0.6006 - binary_accuracy: 0.6338 - val_loss: 1.3862 - val_binary_accuracy: 0.6260\n",
      "Epoch 9/20\n",
      "29/29 - 22s - loss: 0.5990 - binary_accuracy: 0.6332 - val_loss: 1.1737 - val_binary_accuracy: 0.6468\n",
      "Epoch 10/20\n",
      "29/29 - 22s - loss: 0.5919 - binary_accuracy: 0.6351 - val_loss: 1.2191 - val_binary_accuracy: 0.6515\n",
      "Epoch 11/20\n",
      "29/29 - 22s - loss: 0.5916 - binary_accuracy: 0.6512 - val_loss: 1.1032 - val_binary_accuracy: 0.6632\n",
      "Epoch 12/20\n",
      "29/29 - 22s - loss: 0.5821 - binary_accuracy: 0.6523 - val_loss: 1.0278 - val_binary_accuracy: 0.6603\n",
      "Epoch 13/20\n",
      "29/29 - 22s - loss: 0.5809 - binary_accuracy: 0.6569 - val_loss: 1.0638 - val_binary_accuracy: 0.6689\n",
      "Epoch 14/20\n",
      "29/29 - 22s - loss: 0.5898 - binary_accuracy: 0.6588 - val_loss: 1.0075 - val_binary_accuracy: 0.6654\n",
      "Epoch 15/20\n",
      "29/29 - 22s - loss: 0.5787 - binary_accuracy: 0.6639 - val_loss: 0.9654 - val_binary_accuracy: 0.6637\n",
      "Epoch 16/20\n",
      "29/29 - 22s - loss: 0.5964 - binary_accuracy: 0.6501 - val_loss: 1.2652 - val_binary_accuracy: 0.6260\n",
      "Epoch 17/20\n",
      "29/29 - 22s - loss: 0.6017 - binary_accuracy: 0.6444 - val_loss: 1.1394 - val_binary_accuracy: 0.6620\n",
      "Epoch 18/20\n",
      "29/29 - 22s - loss: 0.5768 - binary_accuracy: 0.6547 - val_loss: 1.0271 - val_binary_accuracy: 0.6691\n",
      "Epoch 19/20\n",
      "29/29 - 22s - loss: 0.5830 - binary_accuracy: 0.6517 - val_loss: 1.0931 - val_binary_accuracy: 0.6628\n",
      "Epoch 20/20\n",
      "29/29 - 22s - loss: 0.5780 - binary_accuracy: 0.6648 - val_loss: 1.0112 - val_binary_accuracy: 0.6635\n",
      "14/14 [==============================] - 4s 220ms/step\n"
     ]
    }
   ],
   "source": [
    "node_embeddings = get_node_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到节点嵌入和链接相似性\n",
    "api_embeddings = torch.from_numpy(node_embeddings)\n",
    "ratings = api_embeddings.mm(api_embeddings.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s-%(levelname)s:%(message)s',\n",
    "                    filename='./log/GLAPI-GraphSAGE_AB_'+datasetname+'_'+str(batch_size)+'_'+str(epochs)+'_'+now+'.log',\n",
    "                    filemode='a', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/21 [00:00<00:03,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [00:00<00:03,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [00:01<00:02,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8/21 [00:01<00:02,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [00:01<00:01,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n",
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [00:02<00:01,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17/21 [00:02<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 19/21 [00:03<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "1760\n",
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:03<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "threshold1=0.5\n",
    "threshold2=0.5\n",
    "logger.info('================================================')\n",
    "logger.info('threshold1:%f;threshold2:%f;'%(threshold1,threshold2))\n",
    "myeval(data,ratings,threshold1,threshold2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
